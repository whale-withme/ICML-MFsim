# train_state_transition.py
# è·¯å¾„: MFSim/training/train_state_transition.py

import datetime
import os
import sys
import argparse
import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List
from torch.utils.tensorboard import SummaryWriter

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoTokenizer 

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

# å‡è®¾ encoders å’Œ dataset éƒ½åœ¨æ­£ç¡®ä½ç½®
from model.state_transition.encoders import build_text_encoder
from model.state_transition.state_transition_net import StateTransitionNet
from datasets.state_datasets import StateTransitionDataset 

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainConfig:
    # --- ç›®å½•é…ç½® ---
    event_data_dir: str = "/root/Mean-Field-LLM/mf_llm/data/rumdect/Weibo/test"
    mf_dir: str = "/root/ICML/data/test_mf"
    state_trajectory_dir: str = "/root/ICML/data/test_state_distribution"
    
    # --- å…¨å±€å…±äº«æ–‡ä»¶ ---
    profile_path: str = "/root/ICML/data/profile/cluster_core_user_profile.jsonl"
    uid_mapping_path: str = "/root/ICML/data/profile/user_clusters_map.csv"
    cluster_info_path: str = "/root/ICML/data/profile/cluster_details.json"

    # --- æ¨¡å‹ä¸è®­ç»ƒå‚æ•° ---
    encoder_type: str = "bert"
    model_name: str = "bert-base-chinese"
    text_emb_dim: int = 768
    agent_feat_dim: int = 768
    hidden_dim: int = 256
    use_layernorm: bool = True
    
    # [æ–°å¢] Loss æƒé‡å¹³è¡¡ç³»æ•°
    # alpha * Batch_Loss + (1-alpha) * Global_Loss
    alpha: float = 1 
    
    train_batch_size: int = 32
    max_event: int = 100
    num_agents: int = 16 
    num_epochs: int = 20
    lr: float = 2e-5
    weight_decay: float = 1e-5
    grad_clip: float = 1.0
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    log_interval: int = 10
    save_dir: str = os.path.join(ROOT_DIR, "checkpoints/only_batch") # åœ¨è¿™é‡Œæ”¹ä¿å­˜çš„MLPåœ°å€
    save_name: str = "state_transition_best_batch.pt"

import glob
from torch.utils.data import ConcatDataset

def build_dataloader(cfg: TrainConfig, tokenizer) -> DataLoader:
    # æ„å»ºåŒ…å«æ‰€æœ‰æ–‡ä»¶çš„å·¨å‹ Dataset
    full_dataset = build_full_dataset(cfg)
    
    loader = DataLoader(
        full_dataset,
        batch_size=cfg.train_batch_size,
        shuffle=True, # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼å®ƒä¼šæ‰“ä¹±ä¸åŒäº‹ä»¶é‡Œçš„æ ·æœ¬
        num_workers=10, # æ­¤æ—¶å¯ä»¥å¼€å¯å¤šè¿›ç¨‹åŠ é€Ÿè¯»å–
        pin_memory=True
    )
    
    return loader

def build_full_dataset(cfg: TrainConfig):
    """
    é€»è¾‘å˜æ›´ï¼š
    1. æ‰«æ state_trajectory_dir ä¸‹æ‰€æœ‰çš„ *_trajectory.csv æ–‡ä»¶ (ä½œä¸ºé”šç‚¹)
    2. æå– ID
    3. åå‘æŸ¥æ‰¾å¯¹åº”çš„ .json (raw data) å’Œ _mf.csv (environment context)
    4. å¦‚æœé½å…¨ï¼Œåˆ›å»º Dataset
    5. åˆå¹¶
    """
    
    # 1. ä»¥ Trajectory (çŠ¶æ€åˆ†å¸ƒ GT) æ–‡ä»¶ä¸ºé”šç‚¹è¿›è¡Œæ‰«æ
    traj_pattern = os.path.join(cfg.state_trajectory_dir, "*_trajectory.csv")
    traj_files = glob.glob(traj_pattern)
    
    if not traj_files:
        raise ValueError(f"æœªåœ¨ {cfg.state_trajectory_dir} ä¸‹æ‰¾åˆ°ä»»ä½• *_trajectory.csv æ–‡ä»¶")
    
    traj_files = sorted(traj_files)
    if cfg.max_event is not None and cfg.max_event > 0:
        original_len = len(traj_files)
        traj_files = traj_files[:cfg.max_event]
        print(f"é€‰å–{cfg.max_event}æµ‹è¯•æ–‡ä»¶â€¦â€¦")

    dataset_list = []
    
    # å‡†å¤‡é…ç½®
    file_config = {
        'cluser_user_profile': cfg.profile_path,
        'uid_mapping_path': cfg.uid_mapping_path,
        'cluster_info_path': cfg.cluster_info_path
    }
    
    encoder_config = {
        "type": cfg.encoder_type,
        "model_name": cfg.model_name
    }

    print(f"ğŸ” å¼€å§‹æ‰«æ Trajectory ç›®å½•: {cfg.state_trajectory_dir} ...")
    print(f"   (å…±å‘ç° {len(traj_files)} ä¸ªåˆ†å¸ƒæ–‡ä»¶)")

    for traj_path in traj_files:
        filename = os.path.basename(traj_path)
        
        # 2. æå– ID (å»é™¤åç¼€ _trajectory.csv)
        event_id = filename.replace("_trajectory.csv", "")
        
        # æ’é™¤éæ•°æ®æ–‡ä»¶
        if "cluster" in event_id or "profile" in event_id:
            continue

        # æ ¹æ® ID å»æ‰¾ json
        json_path = os.path.join(cfg.event_data_dir, f"{event_id}.json")
        # æ ¹æ® ID å»æ‰¾ mf.csv
        mf_path = os.path.join(cfg.mf_dir, f"{event_id}_mf.csv")
        
        # 4. æ£€æŸ¥åŸææ–™æ˜¯å¦å­˜åœ¨
        if not os.path.exists(json_path):
            print(f"âš ï¸ è·³è¿‡ {event_id}: æœ‰ Trajectory ä½†ç¼ºå°‘åŸå§‹ JSON æ•°æ® -> {json_path}")
            continue
        if not os.path.exists(mf_path):
            print(f"âš ï¸ è·³è¿‡ {event_id}: æœ‰ Trajectory ä½†ç¼ºå°‘ MF ç¯å¢ƒæ•°æ® -> {mf_path}")
            continue
            
        # 5. å®ä¾‹åŒ–å•ä¸ª Dataset
        try:
            ds = StateTransitionDataset(
                trajectory_path=traj_path,
                mf_path=mf_path,
                test_data_path=json_path,
                profile_path=cfg.profile_path,
                encoder_config=encoder_config,
                file_config=file_config,
                batch_size=cfg.num_agents
            )
            dataset_list.append(ds)
        except Exception as e:
            print(f"âŒ åŠ è½½ {event_id} å¤±è´¥: {e}")

    if not dataset_list:
        raise RuntimeError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æœ‰æ•ˆçš„æ•°æ®é›†ï¼")

    print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset_list)} ä¸ªäº‹ä»¶çš„æ•°æ®é›†")
    
    # 6. åˆå¹¶
    full_dataset = ConcatDataset(dataset_list)
    return full_dataset


def build_models(cfg: TrainConfig):
    # 1. æ–‡æœ¬ç¼–ç å™¨
    encoder_config = {
        "type": cfg.encoder_type,
        "model_name": cfg.model_name,
        "output_dim": cfg.text_emb_dim,
        "freeze": False
    }
    text_encoder = build_text_encoder(encoder_config)

    # 2. çŠ¶æ€è½¬ç§»ç½‘ç»œ
    state_net = StateTransitionNet(
        agent_feat_dim=cfg.agent_feat_dim,
        text_emb_dim=cfg.text_emb_dim,
        hidden_dim=cfg.hidden_dim,
        use_layernorm=cfg.use_layernorm,
    )

    return text_encoder, state_net

def train_one_epoch(
    epoch: int,
    cfg: TrainConfig,
    text_encoder: torch.nn.Module,
    state_net: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    train_loader: DataLoader,
    writer: SummaryWriter
):
    text_encoder.train()
    state_net.train()
    
    total_loss = 0.0
    total_loss_batch = 0.0
    total_loss_global = 0.0
    total_steps = 0

    for batch_idx, batch_data in enumerate(train_loader):
        # 1. è§£åŒ…æ•°æ®å¹¶é€å…¥è®¾å¤‡
        mu_prev = batch_data["mu_prev"].to(cfg.device)        # (B, 3) ä¸Šä¸€æ—¶åˆ»åˆ†å¸ƒ
        target_dist_batch = batch_data["target_dist_batch"].to(cfg.device) # (B, 3) å½“å‰BatchçœŸå®åˆ†å¸ƒ
        
        # [ä¿®æ”¹] ä½¿ç”¨ Reference ä»£ç ä¸­çš„å‘½åï¼Œå¦‚æœæ‚¨çš„ Dataset key æ˜¯ target_dist_sumï¼Œè¯·ä¿æŒå¯¹åº”
        cum_target = batch_data.get("target_dist_sum").to(cfg.device) # (B, 3) çœŸå®å…¨å±€åˆ†å¸ƒ
        
        # [å…³é”®] éœ€è¦ Dataset è¿”å›å½“å‰æ ·æœ¬æ˜¯ç¬¬å‡ æ­¥ (step_idx)ï¼Œç”¨äºè®¡ç®— Global Loss
        step_idx = batch_data["step_idx"].to(cfg.device) # (B,)
        
        agent_feats = batch_data["profile_vecs"].to(cfg.device) # (B, N, D_u)
        mf_texts = batch_data["mf_text"] # List[str]
        
        # 2. å¤„ç†ç¯å¢ƒæ–‡æœ¬ -> Embedding
        tokenizer = text_encoder.tokenizer 
        tokenized_inputs = tokenizer(
            mf_texts, 
            padding=True, 
            truncation=True, 
            max_length=128, 
            return_tensors="pt"
        ).to(cfg.device)
        
        if cfg.encoder_type == 'bert':
            text_emb = text_encoder(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'])
            if isinstance(text_emb, tuple): text_emb = text_emb[0]
        else:
             text_emb = text_encoder(tokenized_inputs['input_ids'])

        # 3. çŠ¶æ€è½¬ç§»ç½‘ç»œå‰å‘ä¼ æ’­
        # mu_pred: å½“å‰ Batch çš„å¾®è§‚é¢„æµ‹
        mu_pred, _ = state_net(mu_prev, text_emb, agent_feats)

        # 4. è®¡ç®—æ··åˆ Loss
        # A. Batch Loss: å¾®è§‚å±‚é¢æ‹Ÿåˆ
        log_mu_pred = torch.log(mu_pred + 1e-8)
        loss_batch = F.kl_div(log_mu_pred, target_dist_batch, reduction='batchmean')

        # B. Global Loss: å®è§‚å±‚é¢æ‹Ÿåˆ (Reference é€»è¾‘)
        # å…¬å¼: pred_global = (step * prev_global_gt + batch_pred) / (step + 1)
        # è§£é‡Š: å‡è®¾ä¸Šä¸€æ—¶åˆ»çš„ç´¯è®¡åˆ†å¸ƒ mu_prev æ˜¯å‡†ç¡®çš„(Ground Truth)ï¼Œæˆ‘ä»¬çœ‹åŠ ä¸Šå½“å‰çš„ mu_pred åï¼Œæ˜¯å¦ç¬¦åˆæ–°çš„ç´¯è®¡åˆ†å¸ƒ
        t = step_idx.unsqueeze(1).float() # (B, 1)
        pred_global = (t * mu_prev + mu_pred) / (t + 1.0)
        loss_global = F.kl_div(torch.log(pred_global + 1e-8), cum_target, reduction='batchmean')

        # C. æœ€ç»ˆåŠ æƒ Loss
        loss = cfg.alpha * loss_batch + (1.0 - cfg.alpha) * loss_global

        # 5. åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        grad_norm = 0.0
        for p in list(text_encoder.parameters()) + list(state_net.parameters()):
            if p.grad is not None:
                grad_norm += p.grad.data.norm(2).item() ** 2
        grad_norm = grad_norm ** 0.5

        if cfg.grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(
                list(text_encoder.parameters()) + list(state_net.parameters()),
                cfg.grad_clip
            )

        optimizer.step()

        # ==========================================================
        # è®¡ç®—è¾…åŠ©æŒ‡æ ‡
        # ==========================================================
        with torch.no_grad():
            # å¾®è§‚æŒ‡æ ‡
            mae_batch = F.l1_loss(mu_pred, target_dist_batch).item()
            pred_label = torch.argmax(mu_pred, dim=1)
            target_label = torch.argmax(target_dist_batch, dim=1)
            acc_batch = (pred_label == target_label).float().mean().item()
            
            # å®è§‚æŒ‡æ ‡ [æ–°å¢]
            mae_global = F.l1_loss(pred_global, cum_target).item()
            global_pred_label = torch.argmax(pred_global, dim=1)
            global_target_label = torch.argmax(cum_target, dim=1)
            acc_global = (global_pred_label == global_target_label).float().mean().item()

        # ==========================================================
        # å†™å…¥ TensorBoard (è¯¦ç»†æ›²çº¿)
        # ==========================================================
        global_step = (epoch - 1) * len(train_loader) + batch_idx
        
        # Loss æ›²çº¿
        writer.add_scalar('Loss/total', loss.item(), global_step)
        writer.add_scalar('Loss/batch_component', loss_batch.item(), global_step)
        writer.add_scalar('Loss/global_component', loss_global.item(), global_step)
        
        # æ€§èƒ½æŒ‡æ ‡
        writer.add_scalar('Metric/MAE_Batch', mae_batch, global_step)
        writer.add_scalar('Metric/MAE_Global', mae_global, global_step)
        writer.add_scalar('Metric/Acc_Batch', acc_batch, global_step)
        writer.add_scalar('Metric/Acc_Global', acc_global, global_step)
        
        # è°ƒè¯•ä¿¡æ¯
        writer.add_scalar('Debug/Grad_Norm', grad_norm, global_step)
        writer.add_scalar('Debug/LR', optimizer.param_groups[0]['lr'], global_step)

        total_loss += loss.item()
        total_loss_batch += loss_batch.item()
        total_loss_global += loss_global.item()
        total_steps += 1

        if (batch_idx + 1) % cfg.log_interval == 0:
            logger.info(
                f"[Epoch {epoch}] Step {batch_idx+1}/{len(train_loader)} | "
                f"L_tot: {loss.item():.4f} (B:{loss_batch.item():.4f}, G:{loss_global.item():.4f}) | "
                f"MAE: {mae_batch:.3f} | Acc: {acc_batch:.1%}"
            )

    avg_loss = total_loss / max(1, total_steps)
    logger.info(f"[Epoch {epoch}] Finished. Avg Loss: {avg_loss:.6f}")
    return avg_loss


def save_checkpoint(cfg, text_encoder, state_net, optimizer, epoch, loss, is_best=False):
    os.makedirs(cfg.save_dir, exist_ok=True)

    ckpt = {
        'epoch': epoch,
        'model_state_dict': state_net.state_dict(),
        'encoder_state_dict': text_encoder.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(), # ä¾ç„¶ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
        'loss': loss,
        'config': str(cfg)
    }

    last_path = os.path.join(cfg.save_dir, "checkpoint_last_batch.pt")
    torch.save(ckpt, last_path)

    if is_best:
        best_path = os.path.join(cfg.save_dir, cfg.save_name)
        torch.save(ckpt, best_path)
        logger.info(f"ğŸŒŸ Best model saved: {best_path} (Loss: {loss:.6f})")
    
    logger.info(f"ğŸ’¾ Checkpoint saved: {last_path}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=32, help="Training batch size")
    parser.add_argument("--resume", action="store_true", help="Resume from last checkpoint")
    args = parser.parse_args()

    # åˆå§‹åŒ–é…ç½®
    cfg = TrainConfig()
    cfg.num_epochs = args.epochs
    cfg.train_batch_size = args.batch_size
    # æ‚¨å¯ä»¥åœ¨è¿™é‡Œè¦†ç›– cfg.alphaï¼Œä¾‹å¦‚ cfg.alpha = 0.6

    beijing_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=8)))
    log_dir = f"checkpoints/runs/run_{beijing_now.strftime('%Y%m%d_%H%M%S')}"
    writer = SummaryWriter(log_dir)
    
    logger.info(f"Device: {cfg.device}")
    
    # 1. æ„å»ºæ¨¡å‹
    text_encoder, state_net = build_models(cfg)
    text_encoder.to(cfg.device)
    state_net.to(cfg.device)
    
    # 2. æ„å»º DataLoader
    train_loader = build_dataloader(cfg, getattr(text_encoder, 'tokenizer', None))

    # 3. ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        list(text_encoder.parameters()) + list(state_net.parameters()),
        lr=cfg.lr,
        weight_decay=cfg.weight_decay
    )

    # 4. è®­ç»ƒå¾ªç¯
    start_epoch = 1
    best_loss = float('inf')
    
    if args.resume:
        ckpt_path = os.path.join(cfg.save_dir, "checkpoint_last.pt")
        if os.path.exists(ckpt_path):
            print(f"ğŸ”„ æ­£åœ¨ä» {ckpt_path} æ¢å¤è®­ç»ƒ...")
            checkpoint = torch.load(ckpt_path, map_location=cfg.device)
            
            # æ¢å¤æ¨¡å‹æƒé‡
            state_net.load_state_dict(checkpoint['model_state_dict'])
            text_encoder.load_state_dict(checkpoint['encoder_state_dict'])
            
            # æ¢å¤ä¼˜åŒ–å™¨
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # æ¢å¤ Epoch
            start_epoch = checkpoint['epoch'] + 1
            best_loss = checkpoint.get('loss', float('inf'))
            
            print(f"âœ… æ¢å¤æˆåŠŸï¼å°†ä» Epoch {start_epoch} å¼€å§‹ç»§ç»­è®­ç»ƒã€‚")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° {ckpt_path}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    for epoch in range(start_epoch, cfg.num_epochs + 1):
        loss = train_one_epoch(epoch, cfg, text_encoder, state_net, optimizer, train_loader, writer)
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯æœ€ä½³
        is_best = loss < best_loss
        if is_best:
            best_loss = loss
            
        save_checkpoint(cfg, text_encoder, state_net, optimizer, epoch, loss, is_best)

    writer.close()

if __name__ == "__main__":
    main()