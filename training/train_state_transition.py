# train_state_transition.py
# è·¯å¾„: MFSim/training/train_state_transition.py

import datetime
import os
import sys
import argparse
import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List
from torch.utils.tensorboard import SummaryWriter

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoTokenizer  # å¦‚æœç”¨ BERT

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

# å‡è®¾ encoders å’Œ dataset éƒ½åœ¨æ­£ç¡®ä½ç½®
from model.state_transition.encoders import build_text_encoder
from model.state_transition.state_transition_net import StateTransitionNet
from datasets import StateTransitionDataset  # å¼•ç”¨ä½ ä¿®æ”¹åçš„ Dataset

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainConfig:
    # --- ç›®å½•é…ç½® ---
    event_data_dir: str = "/root/Mean-Field-LLM/mf_llm/data/rumdect/Weibo/test"
    mf_dir: str = "/root/ICML/data/test_mf"
    state_trajectory_dir: str = "/root/ICML/data/test_state_distribution"
    
    # --- å…¨å±€å…±äº«æ–‡ä»¶ ---
    profile_path: str = "/root/ICML/data/profile/cluster_core_user_profile.jsonl"
    uid_mapping_path: str = "/root/ICML/data/profile/user_clusters_map.csv"
    cluster_info_path: str = "/root/ICML/data/profile/cluster_details.json"

    # --- æ¨¡å‹ä¸è®­ç»ƒå‚æ•° ---
    encoder_type: str = "bert"
    model_name: str = "bert-base-chinese"
    text_emb_dim: int = 768
    agent_feat_dim: int = 768
    hidden_dim: int = 256
    use_layernorm: bool = True
    
    train_batch_size: int = 32
    max_event: int = 100
    num_agents: int = 16 
    num_epochs: int = 20
    lr: float = 2e-5
    weight_decay: float = 1e-5
    grad_clip: float = 1.0
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    log_interval: int = 10
    save_dir: str = os.path.join(ROOT_DIR, "checkpoints")
    save_name: str = "state_transition_best.pt"

import glob
from torch.utils.data import ConcatDataset

def build_dataloader(cfg: TrainConfig, tokenizer) -> DataLoader:
    # æ„å»ºåŒ…å«æ‰€æœ‰æ–‡ä»¶çš„å·¨å‹ Dataset
    full_dataset = build_full_dataset(cfg)
    
    loader = DataLoader(
        full_dataset,
        batch_size=cfg.train_batch_size, # è¿™é‡Œæ˜¯ 32
        shuffle=True, # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼å®ƒä¼šæ‰“ä¹±ä¸åŒäº‹ä»¶é‡Œçš„æ ·æœ¬
        num_workers=4, # æ­¤æ—¶å¯ä»¥å¼€å¯å¤šè¿›ç¨‹åŠ é€Ÿè¯»å–
        pin_memory=True
    )
    
    return loader

def build_full_dataset(cfg: TrainConfig):
    """
    é€»è¾‘å˜æ›´ï¼š
    1. æ‰«æ state_trajectory_dir ä¸‹æ‰€æœ‰çš„ *_trajectory.csv æ–‡ä»¶ (ä½œä¸ºé”šç‚¹)
    2. æå– ID
    3. åå‘æŸ¥æ‰¾å¯¹åº”çš„ .json (raw data) å’Œ _mf.csv (environment context)
    4. å¦‚æœé½å…¨ï¼Œåˆ›å»º Dataset
    5. åˆå¹¶
    """
    
    # 1. ä»¥ Trajectory (çŠ¶æ€åˆ†å¸ƒ GT) æ–‡ä»¶ä¸ºé”šç‚¹è¿›è¡Œæ‰«æ
    # æ³¨æ„ï¼šè¿™é‡Œæ‰«æçš„æ˜¯ state_trajectory_dir
    traj_pattern = os.path.join(cfg.state_trajectory_dir, "*_trajectory.csv")
    traj_files = glob.glob(traj_pattern)
    
    if not traj_files:
        raise ValueError(f"æœªåœ¨ {cfg.state_trajectory_dir} ä¸‹æ‰¾åˆ°ä»»ä½• *_trajectory.csv æ–‡ä»¶")
    
    traj_files = sorted(traj_files)
    if cfg.max_event is not None and cfg.max_event > 0:
        original_len = len(traj_files)
        traj_files = traj_files[:cfg.max_event]
        print(f"é€‰å–{cfg.max_event}æµ‹è¯•æ–‡ä»¶â€¦â€¦")

    dataset_list = []
    
    # å‡†å¤‡é…ç½®
    file_config = {
        'cluser_user_profile': cfg.profile_path,
        'uid_mapping_path': cfg.uid_mapping_path,
        'cluster_info_path': cfg.cluster_info_path
    }
    
    encoder_config = {
        "type": cfg.encoder_type,
        "model_name": cfg.model_name
    }

    print(f"ğŸ” å¼€å§‹æ‰«æ Trajectory ç›®å½•: {cfg.state_trajectory_dir} ...")
    print(f"   (å…±å‘ç° {len(traj_files)} ä¸ªåˆ†å¸ƒæ–‡ä»¶)")

    for traj_path in traj_files:
        # traj_path = ".../4264473811_trajectory.csv"
        filename = os.path.basename(traj_path)  # "4264473811_trajectory.csv"
        
        # 2. æå– ID (å»é™¤åç¼€ _trajectory.csv)
        event_id = filename.replace("_trajectory.csv", "") # "4264473811"
        
        # æ’é™¤éæ•°æ®æ–‡ä»¶
        if "cluster" in event_id or "profile" in event_id:
            continue

        # æ ¹æ® ID å»æ‰¾ json
        json_path = os.path.join(cfg.event_data_dir, f"{event_id}.json")
        # æ ¹æ® ID å»æ‰¾ mf.csv
        mf_path = os.path.join(cfg.mf_dir, f"{event_id}_mf.csv")
        
        # 4. æ£€æŸ¥åŸææ–™æ˜¯å¦å­˜åœ¨
        if not os.path.exists(json_path):
            print(f"âš ï¸ è·³è¿‡ {event_id}: æœ‰ Trajectory ä½†ç¼ºå°‘åŸå§‹ JSON æ•°æ® -> {json_path}")
            continue
        if not os.path.exists(mf_path):
            print(f"âš ï¸ è·³è¿‡ {event_id}: æœ‰ Trajectory ä½†ç¼ºå°‘ MF ç¯å¢ƒæ•°æ® -> {mf_path}")
            continue
            
        # 5. å®ä¾‹åŒ–å•ä¸ª Dataset
        try:
            ds = StateTransitionDataset(
                trajectory_path=traj_path,  # é”šç‚¹æ–‡ä»¶
                mf_path=mf_path,
                test_data_path=json_path,
                profile_path=cfg.profile_path,
                encoder_config=encoder_config,
                file_config=file_config,
                batch_size=cfg.num_agents
            )
            dataset_list.append(ds)
        except Exception as e:
            print(f"âŒ åŠ è½½ {event_id} å¤±è´¥: {e}")

    if not dataset_list:
        raise RuntimeError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æœ‰æ•ˆçš„æ•°æ®é›†ï¼")

    print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset_list)} ä¸ªäº‹ä»¶çš„æ•°æ®é›†")
    
    # 6. åˆå¹¶
    full_dataset = ConcatDataset(dataset_list)
    return full_dataset


def build_models(cfg: TrainConfig):
    # 1. æ–‡æœ¬ç¼–ç å™¨ (ç”¨äºå¤„ç†ç¯å¢ƒæ–‡æœ¬ mf_text)
    encoder_config = {
        "type": cfg.encoder_type,
        "model_name": cfg.model_name,
        "output_dim": cfg.text_emb_dim,
        "freeze": False # è®­ç»ƒæ—¶æ˜¯å¦å¾®è°ƒ BERT
    }
    text_encoder = build_text_encoder(encoder_config)

    # 2. çŠ¶æ€è½¬ç§»ç½‘ç»œ
    state_net = StateTransitionNet(
        agent_feat_dim=cfg.agent_feat_dim,
        text_emb_dim=cfg.text_emb_dim,
        hidden_dim=cfg.hidden_dim,
        use_layernorm=cfg.use_layernorm,
    )

    return text_encoder, state_net

def train_one_epoch(
    epoch: int,
    cfg: TrainConfig,
    text_encoder: torch.nn.Module,
    state_net: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    train_loader: DataLoader,
    writer: SummaryWriter
):
    text_encoder.train()
    state_net.train()
    
    total_loss = 0.0
    total_steps = 0

    for batch_idx, batch_data in enumerate(train_loader):
        # 1. è§£åŒ…æ•°æ®å¹¶é€å…¥è®¾å¤‡
        mu_prev = batch_data["mu_prev"].to(cfg.device)        # (B, 3)
        target_dist = batch_data["target_dist"].to(cfg.device) # (B, 3)
        agent_feats = batch_data["profile_vecs"].to(cfg.device) # (B, N, D_u)
        mf_texts = batch_data["mf_text"] # List[str]
        
        # 2. å¤„ç†ç¯å¢ƒæ–‡æœ¬ -> Embedding
        tokenizer = text_encoder.tokenizer 
        tokenized_inputs = tokenizer(
            mf_texts, 
            padding=True, 
            truncation=True, 
            max_length=128, 
            return_tensors="pt"
        ).to(cfg.device)
        
        if cfg.encoder_type == 'bert':
            text_emb = text_encoder(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'])
            if isinstance(text_emb, tuple): text_emb = text_emb[0]
        else:
             text_emb = text_encoder(tokenized_inputs['input_ids'])

        # 3. çŠ¶æ€è½¬ç§»ç½‘ç»œå‰å‘ä¼ æ’­
        mu_pred, _ = state_net(mu_prev, text_emb, agent_feats)

        # 4. è®¡ç®— Loss (KL Divergence)
        log_mu_pred = torch.log(mu_pred + 1e-8)
        loss = F.kl_div(log_mu_pred, target_dist, reduction='batchmean')

        # ==========================================================
        # [æ–°å¢] è®¡ç®—è¾…åŠ©æŒ‡æ ‡ (ä¸å‚ä¸æ¢¯åº¦å›ä¼ ï¼Œä½¿ç”¨ no_grad)
        # ==========================================================
        with torch.no_grad():
            # A. MAE (å¹³å‡ç»å¯¹è¯¯å·®): ç›´è§‚ç†è§£æ¦‚ç‡åç¦»äº†å¤šå°‘
            mae = F.l1_loss(mu_pred, target_dist).item()

            # B. Trend Accuracy (è¶‹åŠ¿å‡†ç¡®ç‡): é¢„æµ‹çš„ä¸»è¦æ–¹å‘(Pos/Neu/Neg)æ˜¯å¦å¯¹é½
            pred_label = torch.argmax(mu_pred, dim=1)
            target_label = torch.argmax(target_dist, dim=1)
            acc = (pred_label == target_label).float().mean().item()

        # 5. åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        # ==========================================================
        # [æ–°å¢] è®¡ç®—æ¢¯åº¦èŒƒæ•° (ç”¨äºç›‘æ§æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±)
        # ==========================================================
        grad_norm = 0.0
        for p in list(text_encoder.parameters()) + list(state_net.parameters()):
            if p.grad is not None:
                grad_norm += p.grad.data.norm(2).item() ** 2
        grad_norm = grad_norm ** 0.5

        if cfg.grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(
                list(text_encoder.parameters()) + list(state_net.parameters()),
                cfg.grad_clip
            )

        optimizer.step()

        # ==========================================================
        # [æ–°å¢] å†™å…¥ TensorBoard å¤šæ¡æ›²çº¿
        # ==========================================================
        global_step = (epoch - 1) * len(train_loader) + batch_idx
        
        # 1. è®­ç»ƒæŸå¤± (æœ€é‡è¦)
        writer.add_scalar('Loss/train_kl', loss.item(), global_step)
        
        # 2. ä¸šåŠ¡æŒ‡æ ‡ (ç»™äººçœ‹)
        writer.add_scalar('Metric/MAE', mae, global_step)
        writer.add_scalar('Metric/Accuracy', acc, global_step)
        
        # 3. è°ƒè¯•æŒ‡æ ‡ (ç»™å¼€å‘è€…çœ‹)
        writer.add_scalar('Debug/Grad_Norm', grad_norm, global_step)
        # ç›‘æ§å­¦ä¹ ç‡å˜åŒ– (å¦‚æœæ˜¯åŠ¨æ€å­¦ä¹ ç‡çš„è¯å¾ˆæœ‰ç”¨)
        current_lr = optimizer.param_groups[0]['lr']
        writer.add_scalar('Debug/LR', current_lr, global_step)

        total_loss += loss.item()
        total_steps += 1

        if (batch_idx + 1) % cfg.log_interval == 0:
            logger.info(
                f"[Epoch {epoch}] Step {batch_idx+1}/{len(train_loader)} "
                f"Loss: {loss.item():.6f} | MAE: {mae:.4f} | Acc: {acc:.2%} | Grad: {grad_norm:.2f}"
            )

    avg_loss = total_loss / max(1, total_steps)
    logger.info(f"[Epoch {epoch}] Finished. Avg Loss: {avg_loss:.6f}")
    return avg_loss


def save_checkpoint(cfg, text_encoder, state_net, optimizer, epoch, loss, is_best=False):
    os.makedirs(cfg.save_dir, exist_ok=True)

    ckpt = {
        'epoch': epoch,
        'model_state_dict': state_net.state_dict(),
        'encoder_state_dict': text_encoder.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(), # [å…³é”®] ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
        'loss': loss,
        'config': str(cfg)
    }

    last_path = os.path.join(cfg.save_dir, "checkpoint_last.pt")
    torch.save(ckpt, last_path)

    if is_best:
        best_path = os.path.join(cfg.save_dir, cfg.save_name)
        torch.save(ckpt, best_path)
        logger.info(f"ğŸŒŸ Best model saved: {best_path} (Loss: {loss:.6f})")
    
    logger.info(f"ğŸ’¾ Checkpoint saved: {last_path}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=32, help="Training batch size")
    parser.add_argument("--resume", action="store_true", help="Resume from last checkpoint")
    args = parser.parse_args()

    # åˆå§‹åŒ–é…ç½®
    cfg = TrainConfig()
    cfg.num_epochs = args.epochs
    cfg.train_batch_size = args.batch_size

    log_dir = f"checkpoints/runs/run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
    writer = SummaryWriter(log_dir)
    
    logger.info(f"Device: {cfg.device}")
    
    # 1. æ„å»ºæ¨¡å‹
    text_encoder, state_net = build_models(cfg)
    text_encoder.to(cfg.device)
    state_net.to(cfg.device)
    
    # 2. æ„å»º DataLoader
    # ç¡®ä¿ä¼ å…¥æ­£ç¡®çš„å‡½æ•°è°ƒç”¨
    train_loader = build_dataloader(cfg, getattr(text_encoder, 'tokenizer', None))

    # 3. ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        list(text_encoder.parameters()) + list(state_net.parameters()),
        lr=cfg.lr,
        weight_decay=cfg.weight_decay
    )

    # 4. è®­ç»ƒå¾ªç¯
    start_epoch = 1
    best_loss = float('inf')
    
    if args.resume:
        ckpt_path = os.path.join(cfg.save_dir, "checkpoint_last.pt")
        if os.path.exists(ckpt_path):
            print(f"ğŸ”„ æ­£åœ¨ä» {ckpt_path} æ¢å¤è®­ç»ƒ...")
            checkpoint = torch.load(ckpt_path, map_location=cfg.device)
            
            # æ¢å¤æ¨¡å‹æƒé‡
            state_net.load_state_dict(checkpoint['model_state_dict'])
            text_encoder.load_state_dict(checkpoint['encoder_state_dict'])
            
            # æ¢å¤ä¼˜åŒ–å™¨ (è¿™å°±ä¿è¯äº†å­¦ä¹ ç‡å’ŒåŠ¨é‡æ˜¯æ¥ç€ä¸Šæ¬¡çš„)
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # æ¢å¤ Epoch (ä»ä¸‹ä¸€è½®å¼€å§‹)
            start_epoch = checkpoint['epoch'] + 1
            best_loss = checkpoint.get('loss', float('inf')) # å°è¯•è·å–ä¸Šæ¬¡çš„ loss
            
            print(f"âœ… æ¢å¤æˆåŠŸï¼å°†ä» Epoch {start_epoch} å¼€å§‹ç»§ç»­è®­ç»ƒã€‚")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° {ckpt_path}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    # 4. è®­ç»ƒå¾ªç¯
    #range ä» start_epoch å¼€å§‹
    for epoch in range(start_epoch, cfg.num_epochs + 1):
        # è®°å¾—æŠŠ writer ä¼ è¿›å»
        loss = train_one_epoch(epoch, cfg, text_encoder, state_net, optimizer, train_loader, writer)
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯æœ€ä½³
        is_best = loss < best_loss
        if is_best:
            best_loss = loss
            
        # ä¿å­˜ (æ³¨æ„å‚æ•°å˜äº†ï¼Œä¼ å…¥äº† optimizer å’Œ is_best)
        save_checkpoint(cfg, text_encoder, state_net, optimizer, epoch, loss, is_best)

    writer.close()

if __name__ == "__main__":
    main()