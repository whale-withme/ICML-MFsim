# train_state_transition.py
# è·¯å¾„: MFSim/training/train_state_transition.py

import os
import sys
import argparse
import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoTokenizer  # å¦‚æœç”¨ BERT

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

# å‡è®¾ encoders å’Œ dataset éƒ½åœ¨æ­£ç¡®ä½ç½®
from model.state_transition.encoders import build_text_encoder
from model.state_transition.state_transition_net import StateTransitionNet
from datasets import StateTransitionDataset  # å¼•ç”¨ä½ ä¿®æ”¹åçš„ Dataset

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainConfig:
    # --- ç›®å½•é…ç½® ---
    event_data_dir: str = "/root/Mean-Field-LLM/mf_llm/data/rumdect/Weibo/test/tmp"
    mf_dir: str = "/root/ICML/data/test_mf"
    state_trajectory_dir: str = "/root/ICML/data/test_state_distribution"
    
    # --- å…¨å±€å…±äº«æ–‡ä»¶ ---
    profile_path: str = "/root/ICML/data/profile/cluster_core_user_profile.jsonl"
    uid_mapping_path: str = "/root/ICML/data/profile/user_clusters_map.csv"
    cluster_info_path: str = "/root/ICML/data/profile/cluster_details.json"

    # --- æ¨¡å‹ä¸è®­ç»ƒå‚æ•° ---
    encoder_type: str = "bert"
    model_name: str = "bert-base-chinese"
    text_emb_dim: int = 768
    agent_feat_dim: int = 768
    hidden_dim: int = 256
    use_layernorm: bool = True
    
    train_batch_size: int = 32
    num_agents: int = 16 
    num_epochs: int = 20
    lr: float = 2e-5
    weight_decay: float = 1e-5
    grad_clip: float = 1.0
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    log_interval: int = 10
    save_dir: str = os.path.join(ROOT_DIR, "checkpoints")
    save_name: str = "state_transition_best.pt"

import glob
from torch.utils.data import ConcatDataset

def build_dataloader(cfg: TrainConfig, tokenizer) -> DataLoader:
    # æ„å»ºåŒ…å«æ‰€æœ‰æ–‡ä»¶çš„å·¨å‹ Dataset
    full_dataset = build_full_dataset(cfg)
    
    loader = DataLoader(
        full_dataset,
        batch_size=cfg.train_batch_size, # è¿™é‡Œæ˜¯ 32
        shuffle=True, # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼å®ƒä¼šæ‰“ä¹±ä¸åŒäº‹ä»¶é‡Œçš„æ ·æœ¬
        num_workers=4, # æ­¤æ—¶å¯ä»¥å¼€å¯å¤šè¿›ç¨‹åŠ é€Ÿè¯»å–
        pin_memory=True
    )
    
    return loader

def build_full_dataset(cfg: TrainConfig):
    """
    1. æ‰«æ event_data_dir ä¸‹æ‰€æœ‰çš„ .json æ–‡ä»¶ (è§†ä½œ test_data)
    2. æå– ID (ä¾‹å¦‚ 4264473811)
    3. æ£€æŸ¥å¯¹åº”çš„ _trajectory.csv å’Œ _mf.csv æ˜¯å¦å­˜åœ¨
    4. å¦‚æœé½å…¨ï¼Œåˆ›å»ºè¯¥ ID çš„ Dataset
    5. æœ€åç”¨ ConcatDataset æŠŠæ‰€æœ‰ ID çš„ Dataset åˆå¹¶
    """
    
    # 1. æ‰¾åˆ°æ‰€æœ‰ json æ–‡ä»¶ä½œä¸ºé”šç‚¹
    json_pattern = os.path.join(cfg.event_data_dir, "*.json")
    json_files = glob.glob(json_pattern)
    
    if not json_files:
        raise ValueError(f"æœªåœ¨ {cfg.event_data_dir} ä¸‹æ‰¾åˆ°ä»»ä½• .json æ–‡ä»¶")

    dataset_list = []
    
    # å‡†å¤‡ä¼ ç»™ dataset çš„å…¨å±€é…ç½® (profile, uid_map ç­‰)
    file_config = {
        'cluser_user_profile': cfg.profile_path,
        'uid_mapping_path': cfg.uid_mapping_path,
        'cluster_info_path': cfg.cluster_info_path
    }
    
    # Encoder config (å‡è®¾ dataset å†…éƒ¨éœ€è¦)
    encoder_config = {
        "type": cfg.encoder_type,
        "model_name": cfg.model_name
    }

    print(f"ğŸ” å¼€å§‹æ‰«ææ•°æ®ç›®å½•: {cfg.event_data_dir} ...")

    for json_path in json_files:
        # json_path = "data/4264473811.json"
        filename = os.path.basename(json_path)  # "4264473811.json"
        
        # æå– ID (å»é™¤åç¼€)
        event_id = os.path.splitext(filename)[0] # "4264473811"
        
        # æ’é™¤éæ•°æ®æ–‡ä»¶ (æ¯”å¦‚ cluster_info.json å¦‚æœä¹Ÿåœ¨åŒä¸ªç›®å½•)
        if "cluster" in event_id or "profile" in event_id:
            continue

        # 2. è‡ªåŠ¨æ„å»ºå…¶ä»–ä¸¤ä¸ªæ–‡ä»¶çš„è·¯å¾„
        traj_path = os.path.join(cfg.state_trajectory_dir, f"{event_id}_trajectory.csv")
        mf_path = os.path.join(cfg.mf_dir, f"{event_id}_mf.csv")
        
        # 3. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(traj_path):
            print(f"âš ï¸ è·³è¿‡ {event_id}: ç¼ºå°‘ {traj_path}")
            continue
        if not os.path.exists(mf_path):
            print(f"âš ï¸ è·³è¿‡ {event_id}: ç¼ºå°‘ {mf_path}")
            continue
            
        # 4. å®ä¾‹åŒ–å•ä¸ª Dataset
        try:
            ds = StateTransitionDataset(
                trajectory_path=traj_path,
                mf_path=mf_path,
                test_data_path=json_path,
                profile_path=cfg.profile_path,
                encoder_config=encoder_config,
                file_config=file_config,
                batch_size=cfg.num_agents
            )
            dataset_list.append(ds)
        except Exception as e:
            print(f"âŒ åŠ è½½ {event_id} å¤±è´¥: {e}")

    if not dataset_list:
        raise RuntimeError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æœ‰æ•ˆçš„æ•°æ®é›†ï¼")

    print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset_list)} ä¸ªäº‹ä»¶çš„æ•°æ®é›†")
    
    # 5. åˆå¹¶
    full_dataset = ConcatDataset(dataset_list)
    return full_dataset

def build_models(cfg: TrainConfig):
    # 1. æ–‡æœ¬ç¼–ç å™¨ (ç”¨äºå¤„ç†ç¯å¢ƒæ–‡æœ¬ mf_text)
    encoder_config = {
        "type": cfg.encoder_type,
        "model_name": cfg.model_name,
        "output_dim": cfg.text_emb_dim,
        "freeze": False # è®­ç»ƒæ—¶æ˜¯å¦å¾®è°ƒ BERT
    }
    text_encoder = build_text_encoder(encoder_config)

    # 2. çŠ¶æ€è½¬ç§»ç½‘ç»œ
    state_net = StateTransitionNet(
        agent_feat_dim=cfg.agent_feat_dim,
        text_emb_dim=cfg.text_emb_dim,
        hidden_dim=cfg.hidden_dim,
        use_layernorm=cfg.use_layernorm,
    )

    return text_encoder, state_net

def train_one_epoch(
    epoch: int,
    cfg: TrainConfig,
    text_encoder: torch.nn.Module,
    state_net: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    train_loader: DataLoader,
):
    text_encoder.train()
    state_net.train()
    
    total_loss = 0.0
    total_steps = 0

    for batch_idx, batch_data in enumerate(train_loader):
        # 1. è§£åŒ…æ•°æ®å¹¶é€å…¥è®¾å¤‡
        # Dataset è¿”å›å­—å…¸: {"mu_prev": ..., "mf_text": ..., "profile_vecs": ..., "target_dist": ...}
        
        mu_prev = batch_data["mu_prev"].to(cfg.device)        # (B, 3)
        target_dist = batch_data["target_dist"].to(cfg.device) # (B, 3)
        agent_feats = batch_data["profile_vecs"].to(cfg.device) # (B, N, D_u) (B, 16, 768)
        
        mf_texts = batch_data["mf_text"] # List[str] of length B
        
        # 2. å¤„ç†ç¯å¢ƒæ–‡æœ¬ -> Embedding
        # è¿™é‡Œéœ€è¦æ‰‹åŠ¨ Tokenizeï¼Œå› ä¸º Dataset è¿”å›çš„æ˜¯ raw text
        # text_encoder å†…éƒ¨é€šå¸¸å°è£…äº† tokenizer è¿˜æ˜¯åªå°è£…äº† model?
        # å‡è®¾ text_encoder.tokenizer æ˜¯å¯ç”¨çš„ (æ¥è‡ª build_text_encoder)
        
        tokenizer = text_encoder.tokenizer 
        tokenized_inputs = tokenizer(
            mf_texts, 
            padding=True, 
            truncation=True, 
            max_length=128, 
            return_tensors="pt"
        ).to(cfg.device)
        
        # è·å–ç¯å¢ƒæ–‡æœ¬å‘é‡ (B, D_c)
        # å‡è®¾ text_encoder çš„ forward æ¥å— input_ids, attention_mask
        # å¹¶ä¸”è¿”å› pooled output
        if cfg.encoder_type == 'bert':
            # å…¼å®¹ huggingface style
            text_emb = text_encoder(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'])
            if isinstance(text_emb, tuple): text_emb = text_emb[0]
        else:
            # å¦‚æœæ˜¯ GRU ç­‰è‡ªå®šä¹‰ encoder
             text_emb = text_encoder(tokenized_inputs['input_ids'])

        # 3. çŠ¶æ€è½¬ç§»ç½‘ç»œå‰å‘ä¼ æ’­
        # è¾“å…¥: mu_prev(B, 3), text_emb(B, D_c), agent_feats(B, N, D_u)
        # è¾“å‡º: mu_pred(B, 3), probs(B, N, 3)
        mu_pred, _ = state_net(mu_prev, text_emb, agent_feats)

        # 4. è®¡ç®— Loss
        # mseè®¡ç®—å‡å€¼ï¼Œè¿™é‡Œå…ˆç”¨KLç®—
        # loss = F.mse_loss(mu_pred, target_dist)
        log_mu_pred = torch.log(mu_pred + 1e-8) # åŠ å¾®å°å€¼é˜²æ­¢ log(0)
        loss = F.kl_div(log_mu_pred, target_dist, reduction='batchmean')

        # 5. åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        if cfg.grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(
                list(text_encoder.parameters()) + list(state_net.parameters()),
                cfg.grad_clip
            )

        optimizer.step()

        total_loss += loss.item()
        total_steps += 1

        if (batch_idx + 1) % cfg.log_interval == 0:
            logger.info(
                f"[Epoch {epoch}] Step {batch_idx+1}/{len(train_loader)} "
                f"Loss: {loss.item():.6f} (Avg: {total_loss/total_steps:.6f})"
            )

    avg_loss = total_loss / max(1, total_steps)
    logger.info(f"[Epoch {epoch}] Finished. Avg Loss: {avg_loss:.6f}")
    return avg_loss

def save_checkpoint(cfg, text_encoder, state_net, epoch, loss):
    os.makedirs(cfg.save_dir, exist_ok=True)
    save_path = os.path.join(cfg.save_dir, cfg.save_name)
    torch.save({
        'epoch': epoch,
        'model_state_dict': state_net.state_dict(),
        'encoder_state_dict': text_encoder.state_dict(),
        'loss': loss,
        'config': str(cfg)
    }, save_path)
    logger.info(f"Checkpoint saved: {save_path}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=32, help="Training batch size")
    args = parser.parse_args()

    # åˆå§‹åŒ–é…ç½®
    cfg = TrainConfig()
    cfg.num_epochs = args.epochs
    cfg.train_batch_size = args.batch_size
    
    logger.info(f"Device: {cfg.device}")
    
    # 1. æ„å»ºæ¨¡å‹
    text_encoder, state_net = build_models(cfg)
    text_encoder.to(cfg.device)
    state_net.to(cfg.device)
    
    # 2. æ„å»º DataLoader
    # ç¡®ä¿ä¼ å…¥æ­£ç¡®çš„å‡½æ•°è°ƒç”¨
    train_loader = build_dataloader(cfg, getattr(text_encoder, 'tokenizer', None))

    # 3. ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        list(text_encoder.parameters()) + list(state_net.parameters()),
        lr=cfg.lr,
        weight_decay=cfg.weight_decay
    )

    # 4. è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    for epoch in range(1, cfg.num_epochs + 1):
        loss = train_one_epoch(epoch, cfg, text_encoder, state_net, optimizer, train_loader)
        
        if loss < best_loss:
            best_loss = loss
            save_checkpoint(cfg, text_encoder, state_net, epoch, loss)

if __name__ == "__main__":
    main()