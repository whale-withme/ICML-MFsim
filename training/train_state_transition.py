# train_state_transition.py
# è·¯å¾„: MFSim/training/train_state_transition.py

import os
import sys
import argparse
import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List
from torch.utils.tensorboard import SummaryWriter

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoTokenizer  # å¦‚æœç”¨ BERT

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

# å‡è®¾ encoders å’Œ dataset éƒ½åœ¨æ­£ç¡®ä½ç½®
from model.state_transition.encoders import build_text_encoder
from model.state_transition.state_transition_net import StateTransitionNet
from datasets import StateTransitionDataset  # å¼•ç”¨ä½ ä¿®æ”¹åçš„ Dataset

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainConfig:
    # --- ç›®å½•é…ç½® ---
    event_data_dir: str = "/root/Mean-Field-LLM/mf_llm/data/rumdect/Weibo/test"
    mf_dir: str = "/root/ICML/data/test_mf"
    state_trajectory_dir: str = "/root/ICML/data/test_state_distribution"
    
    # --- å…¨å±€å…±äº«æ–‡ä»¶ ---
    profile_path: str = "/root/ICML/data/profile/cluster_core_user_profile.jsonl"
    uid_mapping_path: str = "/root/ICML/data/profile/user_clusters_map.csv"
    cluster_info_path: str = "/root/ICML/data/profile/cluster_details.json"

    # --- æ¨¡å‹ä¸è®­ç»ƒå‚æ•° ---
    encoder_type: str = "bert"
    model_name: str = "bert-base-chinese"
    text_emb_dim: int = 768
    agent_feat_dim: int = 768
    hidden_dim: int = 256
    use_layernorm: bool = True
    
    train_batch_size: int = 32
    max_event: int = 2
    num_agents: int = 16 
    num_epochs: int = 20
    lr: float = 2e-5
    weight_decay: float = 1e-5
    grad_clip: float = 1.0
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    log_interval: int = 10
    save_dir: str = os.path.join(ROOT_DIR, "checkpoints")
    save_name: str = "state_transition_best.pt"

import glob
from torch.utils.data import ConcatDataset

def build_dataloader(cfg: TrainConfig, tokenizer) -> DataLoader:
    # æ„å»ºåŒ…å«æ‰€æœ‰æ–‡ä»¶çš„å·¨å‹ Dataset
    full_dataset = build_full_dataset(cfg)
    
    loader = DataLoader(
        full_dataset,
        batch_size=cfg.train_batch_size, # è¿™é‡Œæ˜¯ 32
        shuffle=True, # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼å®ƒä¼šæ‰“ä¹±ä¸åŒäº‹ä»¶é‡Œçš„æ ·æœ¬
        num_workers=4, # æ­¤æ—¶å¯ä»¥å¼€å¯å¤šè¿›ç¨‹åŠ é€Ÿè¯»å–
        pin_memory=True
    )
    
    return loader

def build_full_dataset(cfg: TrainConfig):
    """
    é€»è¾‘å˜æ›´ï¼š
    1. æ‰«æ state_trajectory_dir ä¸‹æ‰€æœ‰çš„ *_trajectory.csv æ–‡ä»¶ (ä½œä¸ºé”šç‚¹)
    2. æå– ID
    3. åå‘æŸ¥æ‰¾å¯¹åº”çš„ .json (raw data) å’Œ _mf.csv (environment context)
    4. å¦‚æœé½å…¨ï¼Œåˆ›å»º Dataset
    5. åˆå¹¶
    """
    
    # 1. ä»¥ Trajectory (çŠ¶æ€åˆ†å¸ƒ GT) æ–‡ä»¶ä¸ºé”šç‚¹è¿›è¡Œæ‰«æ
    # æ³¨æ„ï¼šè¿™é‡Œæ‰«æçš„æ˜¯ state_trajectory_dir
    traj_pattern = os.path.join(cfg.state_trajectory_dir, "*_trajectory.csv")
    traj_files = glob.glob(traj_pattern)
    
    if not traj_files:
        raise ValueError(f"æœªåœ¨ {cfg.state_trajectory_dir} ä¸‹æ‰¾åˆ°ä»»ä½• *_trajectory.csv æ–‡ä»¶")
    
    traj_files = sorted(traj_files)
    if cfg.max_event is not None and cfg.max_event > 0:
        original_len = len(traj_files)
        traj_files = traj_files[:cfg.max_event]
        print(f"é€‰å–{cfg.max_event}æµ‹è¯•æ–‡ä»¶â€¦â€¦")

    dataset_list = []
    
    # å‡†å¤‡é…ç½®
    file_config = {
        'cluser_user_profile': cfg.profile_path,
        'uid_mapping_path': cfg.uid_mapping_path,
        'cluster_info_path': cfg.cluster_info_path
    }
    
    encoder_config = {
        "type": cfg.encoder_type,
        "model_name": cfg.model_name
    }

    print(f"ğŸ” å¼€å§‹æ‰«æ Trajectory ç›®å½•: {cfg.state_trajectory_dir} ...")
    print(f"   (å…±å‘ç° {len(traj_files)} ä¸ªåˆ†å¸ƒæ–‡ä»¶)")

    for traj_path in traj_files:
        # traj_path = ".../4264473811_trajectory.csv"
        filename = os.path.basename(traj_path)  # "4264473811_trajectory.csv"
        
        # 2. æå– ID (å»é™¤åç¼€ _trajectory.csv)
        event_id = filename.replace("_trajectory.csv", "") # "4264473811"
        
        # æ’é™¤éæ•°æ®æ–‡ä»¶
        if "cluster" in event_id or "profile" in event_id:
            continue

        # æ ¹æ® ID å»æ‰¾ json
        json_path = os.path.join(cfg.event_data_dir, f"{event_id}.json")
        # æ ¹æ® ID å»æ‰¾ mf.csv
        mf_path = os.path.join(cfg.mf_dir, f"{event_id}_mf.csv")
        
        # 4. æ£€æŸ¥åŸææ–™æ˜¯å¦å­˜åœ¨
        if not os.path.exists(json_path):
            print(f"âš ï¸ è·³è¿‡ {event_id}: æœ‰ Trajectory ä½†ç¼ºå°‘åŸå§‹ JSON æ•°æ® -> {json_path}")
            continue
        if not os.path.exists(mf_path):
            print(f"âš ï¸ è·³è¿‡ {event_id}: æœ‰ Trajectory ä½†ç¼ºå°‘ MF ç¯å¢ƒæ•°æ® -> {mf_path}")
            continue
            
        # 5. å®ä¾‹åŒ–å•ä¸ª Dataset
        try:
            ds = StateTransitionDataset(
                trajectory_path=traj_path,  # é”šç‚¹æ–‡ä»¶
                mf_path=mf_path,
                test_data_path=json_path,
                profile_path=cfg.profile_path,
                encoder_config=encoder_config,
                file_config=file_config,
                batch_size=cfg.num_agents
            )
            dataset_list.append(ds)
        except Exception as e:
            print(f"âŒ åŠ è½½ {event_id} å¤±è´¥: {e}")

    if not dataset_list:
        raise RuntimeError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æœ‰æ•ˆçš„æ•°æ®é›†ï¼")

    print(f"âœ… æˆåŠŸåŠ è½½ {len(dataset_list)} ä¸ªäº‹ä»¶çš„æ•°æ®é›†")
    
    # 6. åˆå¹¶
    full_dataset = ConcatDataset(dataset_list)
    return full_dataset


def build_models(cfg: TrainConfig):
    # 1. æ–‡æœ¬ç¼–ç å™¨ (ç”¨äºå¤„ç†ç¯å¢ƒæ–‡æœ¬ mf_text)
    encoder_config = {
        "type": cfg.encoder_type,
        "model_name": cfg.model_name,
        "output_dim": cfg.text_emb_dim,
        "freeze": False # è®­ç»ƒæ—¶æ˜¯å¦å¾®è°ƒ BERT
    }
    text_encoder = build_text_encoder(encoder_config)

    # 2. çŠ¶æ€è½¬ç§»ç½‘ç»œ
    state_net = StateTransitionNet(
        agent_feat_dim=cfg.agent_feat_dim,
        text_emb_dim=cfg.text_emb_dim,
        hidden_dim=cfg.hidden_dim,
        use_layernorm=cfg.use_layernorm,
    )

    return text_encoder, state_net

def train_one_epoch(
    epoch: int,
    cfg: TrainConfig,
    text_encoder: torch.nn.Module,
    state_net: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    train_loader: DataLoader,
    writer: SummaryWriter
):
    text_encoder.train()
    state_net.train()
    
    total_loss = 0.0
    total_steps = 0

    for batch_idx, batch_data in enumerate(train_loader):
        # 1. è§£åŒ…æ•°æ®å¹¶é€å…¥è®¾å¤‡
        # Dataset è¿”å›å­—å…¸: {"mu_prev": ..., "mf_text": ..., "profile_vecs": ..., "target_dist": ...}
        
        mu_prev = batch_data["mu_prev"].to(cfg.device)        # (B, 3)
        target_dist = batch_data["target_dist"].to(cfg.device) # (B, 3)
        agent_feats = batch_data["profile_vecs"].to(cfg.device) # (B, N, D_u) (B, 16, 768)
        
        mf_texts = batch_data["mf_text"] # List[str] of length B
        
        # 2. å¤„ç†ç¯å¢ƒæ–‡æœ¬ -> Embedding
        # è¿™é‡Œéœ€è¦æ‰‹åŠ¨ Tokenizeï¼Œå› ä¸º Dataset è¿”å›çš„æ˜¯ raw text
        # text_encoder å†…éƒ¨é€šå¸¸å°è£…äº† tokenizer è¿˜æ˜¯åªå°è£…äº† model?
        # å‡è®¾ text_encoder.tokenizer æ˜¯å¯ç”¨çš„ (æ¥è‡ª build_text_encoder)
        
        tokenizer = text_encoder.tokenizer 
        tokenized_inputs = tokenizer(
            mf_texts, 
            padding=True, 
            truncation=True, 
            max_length=128, 
            return_tensors="pt"
        ).to(cfg.device)
        
        # è·å–ç¯å¢ƒæ–‡æœ¬å‘é‡ (B, D_c)
        # å‡è®¾ text_encoder çš„ forward æ¥å— input_ids, attention_mask
        # å¹¶ä¸”è¿”å› pooled output
        if cfg.encoder_type == 'bert':
            # å…¼å®¹ huggingface style
            text_emb = text_encoder(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'])
            if isinstance(text_emb, tuple): text_emb = text_emb[0]
        else:
            # å¦‚æœæ˜¯ GRU ç­‰è‡ªå®šä¹‰ encoder
             text_emb = text_encoder(tokenized_inputs['input_ids'])

        # 3. çŠ¶æ€è½¬ç§»ç½‘ç»œå‰å‘ä¼ æ’­
        # è¾“å…¥: mu_prev(B, 3), text_emb(B, D_c), agent_feats(B, N, D_u)
        # è¾“å‡º: mu_pred(B, 3), probs(B, N, 3)
        mu_pred, _ = state_net(mu_prev, text_emb, agent_feats)

        # 4. è®¡ç®— Loss
        # mseè®¡ç®—å‡å€¼ï¼Œè¿™é‡Œå…ˆç”¨KLç®—
        # loss = F.mse_loss(mu_pred, target_dist)
        log_mu_pred = torch.log(mu_pred + 1e-8) # åŠ å¾®å°å€¼é˜²æ­¢ log(0)
        loss = F.kl_div(log_mu_pred, target_dist, reduction='batchmean')

        # 5. åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        if cfg.grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(
                list(text_encoder.parameters()) + list(state_net.parameters()),
                cfg.grad_clip
            )

        optimizer.step()

        # å¢åŠ lossè®°å½•
        global_step = (epoch - 1) * len(train_loader) + batch_idx
        writer.add_scalar('Loss/train', loss.item(), global_step)

        total_loss += loss.item()
        total_steps += 1

        if (batch_idx + 1) % cfg.log_interval == 0:
            logger.info(
                f"[Epoch {epoch}] Step {batch_idx+1}/{len(train_loader)} "
                f"Loss: {loss.item():.6f} (Avg: {total_loss/total_steps:.6f})"
            )

    avg_loss = total_loss / max(1, total_steps)
    logger.info(f"[Epoch {epoch}] Finished. Avg Loss: {avg_loss:.6f}")
    return avg_loss

def save_checkpoint(cfg, text_encoder, state_net, epoch, loss):
    os.makedirs(cfg.save_dir, exist_ok=True)
    save_path = os.path.join(cfg.save_dir, cfg.save_name)
    torch.save({
        'epoch': epoch,
        'model_state_dict': state_net.state_dict(),
        'encoder_state_dict': text_encoder.state_dict(),
        'loss': loss,
        'config': str(cfg)
    }, save_path)
    logger.info(f"Checkpoint saved: {save_path}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=32, help="Training batch size")
    args = parser.parse_args()

    # åˆå§‹åŒ–é…ç½®
    cfg = TrainConfig()
    cfg.num_epochs = args.epochs
    cfg.train_batch_size = args.batch_size
    writer = SummaryWriter(log_dir=os.path.join(cfg.save_dir, 'runs'))
    
    logger.info(f"Device: {cfg.device}")
    
    # 1. æ„å»ºæ¨¡å‹
    text_encoder, state_net = build_models(cfg)
    text_encoder.to(cfg.device)
    state_net.to(cfg.device)
    
    # 2. æ„å»º DataLoader
    # ç¡®ä¿ä¼ å…¥æ­£ç¡®çš„å‡½æ•°è°ƒç”¨
    train_loader = build_dataloader(cfg, getattr(text_encoder, 'tokenizer', None))

    # 3. ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        list(text_encoder.parameters()) + list(state_net.parameters()),
        lr=cfg.lr,
        weight_decay=cfg.weight_decay
    )

    # 4. è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    for epoch in range(1, cfg.num_epochs + 1):
        loss = train_one_epoch(epoch, cfg, text_encoder, state_net, optimizer, train_loader, writer)
        
        if loss < best_loss:
            best_loss = loss
            save_checkpoint(cfg, text_encoder, state_net, epoch, loss)

    writer.close()

if __name__ == "__main__":
    main()